{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEctq5gVRk7C"
      },
      "source": [
        "\n",
        "###1. Для начала определим широкий топик текста с помощью gensim."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcpF3Fn79bqY"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONW1vJ919hEJ"
      },
      "source": [
        "!pip install spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpIsCv5m9_A6"
      },
      "source": [
        "!pip  install pyLDAvis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Filfs7op8WkY"
      },
      "source": [
        "import nltk; nltk.download('stopwords')\n",
        "\n",
        "\n",
        "!python3 -m spacy download en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsxRoSGK9zBL"
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "# Gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# spacy for lemmatization\n",
        "import spacy\n",
        "\n",
        "# Plotting tools\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim  # don't skip this\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Enable logging for gensim - optional\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYWmtv1M91nu"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAyh7LTgAVch",
        "outputId": "365c308c-de7b-4ccc-dcb9-0ed4e31af6bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        }
      },
      "source": [
        "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
        "print(df.target_names.unique())\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['rec.autos' 'comp.sys.mac.hardware' 'comp.graphics' 'sci.space'\n",
            " 'talk.politics.guns' 'sci.med' 'comp.sys.ibm.pc.hardware'\n",
            " 'comp.os.ms-windows.misc' 'rec.motorcycles' 'talk.religion.misc'\n",
            " 'misc.forsale' 'alt.atheism' 'sci.electronics' 'comp.windows.x'\n",
            " 'rec.sport.hockey' 'rec.sport.baseball' 'soc.religion.christian'\n",
            " 'talk.politics.mideast' 'talk.politics.misc' 'sci.crypt']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>target</th>\n",
              "      <th>target_names</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
              "      <td>7</td>\n",
              "      <td>rec.autos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
              "      <td>4</td>\n",
              "      <td>comp.sys.mac.hardware</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
              "      <td>4</td>\n",
              "      <td>comp.sys.mac.hardware</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
              "      <td>1</td>\n",
              "      <td>comp.graphics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
              "      <td>14</td>\n",
              "      <td>sci.space</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             content  ...           target_names\n",
              "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...  ...              rec.autos\n",
              "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...  ...  comp.sys.mac.hardware\n",
              "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...  ...  comp.sys.mac.hardware\n",
              "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...  ...          comp.graphics\n",
              "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...  ...              sci.space\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_DJRWg8BGsW",
        "outputId": "c3493284-7e66-4685-91ad-f4c0e7371bc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "data = df.content.values.tolist()\n",
        "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
        "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
        "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
        "pprint(data[:1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['From: (wheres my thing) Subject: WHAT car is this!? Nntp-Posting-Host: '\n",
            " 'rac3.wam.umd.edu Organization: University of Maryland, College Park Lines: '\n",
            " '15 I was wondering if anyone out there could enlighten me on this car I saw '\n",
            " 'the other day. It was a 2-door sports car, looked to be from the late 60s/ '\n",
            " 'early 70s. It was called a Bricklin. The doors were really small. In '\n",
            " 'addition, the front bumper was separate from the rest of the body. This is '\n",
            " 'all I know. If anyone can tellme a model name, engine specs, years of '\n",
            " 'production, where this car is made, history, or whatever info you have on '\n",
            " 'this funky looking car, please e-mail. Thanks, - IL ---- brought to you by '\n",
            " 'your neighborhood Lerxst ---- ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE4tBFnlBa_v",
        "outputId": "7e445fe4-0e3e-4e99-cf18-aebe429b4de4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "#токенизируем, делаем из каждого предложения список\n",
        "\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
        "\n",
        "data_words = list(sent_to_words(data))\n",
        "\n",
        "print(data_words[:1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['from', 'wheres', 'my', 'thing', 'subject', 'what', 'car', 'is', 'this', 'nntp', 'posting', 'host', 'rac', 'wam', 'umd', 'edu', 'organization', 'university', 'of', 'maryland', 'college', 'park', 'lines', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'saw', 'the', 'other', 'day', 'it', 'was', 'door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late', 'early', 'it', 'was', 'called', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'in', 'addition', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', 'this', 'is', 'all', 'know', 'if', 'anyone', 'can', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'of', 'production', 'where', 'this', 'car', 'is', 'made', 'history', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', 'please', 'mail', 'thanks', 'il', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'lerxst']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqxJzKq5B07B",
        "outputId": "02c955f4-4b26-4b4c-a065-301326a074a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# Build the bigram and trigram models\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
        "\n",
        "# Faster way to get a sentence clubbed as a trigram/bigram\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "\n",
        "# See trigram example\n",
        "print(trigram_mod[bigram_mod[data_words[0]]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['from', 'wheres', 'my', 'thing', 'subject', 'what', 'car', 'is', 'this', 'nntp_posting_host', 'rac_wam_umd_edu', 'organization', 'university', 'of', 'maryland_college_park', 'lines', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'saw', 'the', 'other', 'day', 'it', 'was', 'door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late', 'early', 'it', 'was', 'called', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'in', 'addition', 'the', 'front_bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', 'this', 'is', 'all', 'know', 'if', 'anyone', 'can', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'of', 'production', 'where', 'this', 'car', 'is', 'made', 'history', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', 'please', 'mail', 'thanks', 'il', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'lerxst']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLY1GmA0BbmB"
      },
      "source": [
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def make_trigrams(texts):\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rE1F6oMECSvm",
        "outputId": "e168e88a-5157-48f4-dd48-917c2839b0d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# Remove Stop Words\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "# Form Bigrams\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
        "# python3 -m spacy download en\n",
        "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "\n",
        "# Do lemmatization keeping only noun, adj, vb, adv\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "print(data_lemmatized[:1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['where', 'thing', 'car', 'nntp_poste', 'host', 'park', 'line', 'wonder', 'could', 'enlighten', 'car', 'see', 'day', 'door', 'sport', 'car', 'look', 'late', 'early', 'call', 'bricklin', 'door', 'really', 'small', 'addition', 'separate', 'rest', 'body', 'know', 'tellme', 'model', 'name', 'engine', 'year', 'production', 'car', 'make', 'history', 'info', 'funky', 'look', 'car', 'mail', 'thank', 'bring', 'neighborhood', 'lerxst']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0Wa02IHC59A"
      },
      "source": [
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "\n",
        "# Create Corpus\n",
        "texts = data_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uhwy16jWD9-6"
      },
      "source": [
        "####Тренеруем нашу модель"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBmqT7oLD9YP"
      },
      "source": [
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=11, \n",
        "                                           random_state=100,\n",
        "                                           update_every=2,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           alpha='auto',\n",
        "                                           per_word_topics=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70i5r5TFD6M5",
        "outputId": "da1510df-6585-4df6-c9bf-a9b126f395ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 774
        }
      },
      "source": [
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0,\n",
            "  '0.029*\"team\" + 0.026*\"game\" + 0.022*\"year\" + 0.019*\"play\" + 0.016*\"player\" '\n",
            "  '+ 0.014*\"win\" + 0.010*\"season\" + 0.009*\"run\" + 0.009*\"last\" + '\n",
            "  '0.008*\"score\"'),\n",
            " (1,\n",
            "  '0.162*\"ax\" + 0.137*\"max\" + 0.012*\"pen\" + 0.011*\"di_di\" + 0.008*\"patient\" + '\n",
            "  '0.008*\"registration\" + 0.008*\"inch\" + 0.007*\"penguin\" + 0.007*\"leafs\" + '\n",
            "  '0.005*\"cub\"'),\n",
            " (2,\n",
            "  '0.015*\"space\" + 0.006*\"launch\" + 0.006*\"engine\" + 0.006*\"high\" + '\n",
            "  '0.006*\"system\" + 0.006*\"food\" + 0.005*\"low\" + 0.005*\"mission\" + '\n",
            "  '0.005*\"test\" + 0.005*\"earth\"'),\n",
            " (3,\n",
            "  '0.040*\"drive\" + 0.025*\"car\" + 0.011*\"bike\" + 0.010*\"power\" + 0.008*\"wire\" + '\n",
            "  '0.008*\"use\" + 0.008*\"light\" + 0.008*\"scsi\" + 0.007*\"ride\" + 0.006*\"high\"'),\n",
            " (4,\n",
            "  '0.016*\"gun\" + 0.016*\"kill\" + 0.010*\"attack\" + 0.010*\"israeli\" + '\n",
            "  '0.010*\"child\" + 0.010*\"war\" + 0.010*\"drug\" + 0.009*\"fire\" + 0.008*\"report\" '\n",
            "  '+ 0.007*\"death\"'),\n",
            " (5,\n",
            "  '0.008*\"say\" + 0.007*\"people\" + 0.007*\"slave\" + 0.007*\"year\" + 0.005*\"cover\" '\n",
            "  '+ 0.005*\"turkish\" + 0.005*\"leave\" + 0.005*\"work\" + 0.005*\"take\" + '\n",
            "  '0.005*\"woman\"'),\n",
            " (6,\n",
            "  '0.020*\"use\" + 0.019*\"window\" + 0.018*\"key\" + 0.017*\"system\" + 0.015*\"bit\" + '\n",
            "  '0.015*\"card\" + 0.012*\"chip\" + 0.011*\"run\" + 0.010*\"driver\" + '\n",
            "  '0.010*\"problem\"'),\n",
            " (7,\n",
            "  '0.016*\"government\" + 0.013*\"public\" + 0.011*\"law\" + 0.011*\"state\" + '\n",
            "  '0.011*\"right\" + 0.007*\"people\" + 0.007*\"issue\" + 0.007*\"encryption\" + '\n",
            "  '0.006*\"protect\" + 0.006*\"security\"'),\n",
            " (8,\n",
            "  '0.011*\"say\" + 0.010*\"believe\" + 0.010*\"people\" + 0.008*\"evidence\" + '\n",
            "  '0.007*\"exist\" + 0.007*\"religion\" + 0.007*\"may\" + 0.007*\"claim\" + '\n",
            "  '0.006*\"reason\" + 0.006*\"argument\"'),\n",
            " (9,\n",
            "  '0.023*\"line\" + 0.014*\"file\" + 0.013*\"program\" + 0.012*\"thank\" + '\n",
            "  '0.011*\"mail\" + 0.009*\"include\" + 0.009*\"also\" + 0.008*\"information\" + '\n",
            "  '0.008*\"use\" + 0.008*\"host\"'),\n",
            " (10,\n",
            "  '0.030*\"would\" + 0.026*\"write\" + 0.021*\"line\" + 0.017*\"article\" + '\n",
            "  '0.016*\"know\" + 0.015*\"be\" + 0.014*\"think\" + 0.014*\"go\" + 0.013*\"say\" + '\n",
            "  '0.013*\"make\"')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H72tN5WJSYa1"
      },
      "source": [
        "###2. Создайте функцию или серию функций, через которую будет удобно подобрать оптимальное число групп - 1 балл за нахождение оптимального числа групп, 1 балл - если это будет не руками, а через функцию"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "050OfYs81FPa"
      },
      "source": [
        "Просто будем считать coherence score для каждого числа групп"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jx1D5C-6SkWU"
      },
      "source": [
        "def optimal_q(corpus, id2word, data_lemmatized):\n",
        "  coh_numTopics = {}\n",
        "  for n in range(8,30):\n",
        "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=n, \n",
        "                                           random_state=100,\n",
        "                                           update_every=2,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           alpha='auto',\n",
        "                                           per_word_topics=True)\n",
        "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "    coh = coherence_model_lda.get_coherence()\n",
        "    coh_numTopics[coh] = n\n",
        "  return coh_numTopics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9gbEJeYY_m4"
      },
      "source": [
        "coh_numTopics = optimal_q(corpus, id2word, data_lemmatized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6P1Nz-ys6sl",
        "outputId": "5fa78fc6-261f-4c6f-8650-f318cb82bbfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "max(coh_numTopics.items())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5363387710830442, 11)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOftOEtx1Qhb"
      },
      "source": [
        "Получается, оптимальное число групп - 11"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-SlPAebZ_8u"
      },
      "source": [
        "###3. Как вы знаете, gensim считает, что каждый текст содержит несколько топиков, но на следующем этапе вам будет надо создать функцию, которая будет для каждого текста определять один широкий топик, самый главный. Мы предлагаем сделать это так: создайте счётчик, и каждый раз, когда в тексте будет встречаться одно из слов, соответсвующих данной теме, добавляйте к счетчику его вес. 2 балла"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aG53S830iMxx"
      },
      "source": [
        "topics = lda_model.show_topics(formatted=False, num_words= 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_63LACKzbTBt"
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOVNR6LP1hpv"
      },
      "source": [
        "Сделаем все как в инструкции)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAGIqgMNJMNs"
      },
      "source": [
        "text_topic = {}\n",
        "id_t = 0\n",
        "for text in texts:\n",
        "  id_t +=1\n",
        "  wide_topics = Counter()\n",
        "  for w in text:\n",
        "    for topic in topics:\n",
        "      for t in topic[1]:\n",
        "        if t[0] == w:\n",
        "          wide_topics[topic[0]] += float(t[1])\n",
        "  text_topic[' '.join(text)] = wide_topics.most_common(1)\n",
        "#print(text_topic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpsSFDD41nty"
      },
      "source": [
        "Запишем в новый словарь только текст и номер темы, чтобы красиво записать в дата фрейм"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GryXYJgFzWNN"
      },
      "source": [
        "text_topic1 = {}\n",
        "for text, topic in text_topic.items():\n",
        "  if topic != []:\n",
        "    text_topic1[text] = topic[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zh-vD4d2tzpM"
      },
      "source": [
        "colNames = ['topic']\n",
        "df = pd.DataFrame.from_dict(text_topic1, orient='index', columns=colNames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOU3hGobumY1",
        "outputId": "79d2192c-3ccb-41ce-a966-b3d6562483be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "source": [
        "print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                    topic\n",
            "where thing car nntp_poste host park line wonde...      3\n",
            "poll final call summary final call clock report...      9\n",
            "engineering computer network distribution_usa l...     10\n",
            "division line host write write article know chi...     10\n",
            "question distribution article write clear cauti...     10\n",
            "...                                                   ...\n",
            "scan city reply line consultation cheap also we...     10\n",
            "screen medford old problem screen blank sometim...      3\n",
            "este mount mail group line instal try mount coo...      9\n",
            "line article write boy embarasse trivial faq gi...     10\n",
            "steal organization line distribution_usa host s...      9\n",
            "\n",
            "[11279 rows x 1 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xK0bMAh813Wy"
      },
      "source": [
        "Запишем его в файл"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTkijTg70PsR"
      },
      "source": [
        "df.to_csv('text-topic.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N45v5VVMvquH"
      },
      "source": [
        "###4. После того, как у вас получится какое-то количество групп (наборов текстов с общим топиком). Внутри каждой из этих групп посчитайте тф_идф для каждого текста (Т. е. возьмите все тексты с одинаковой темой за ваш корпус и посчитайте для каждого из этих текстов тф_идф). 3 балла"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyf4ixbQ18FQ"
      },
      "source": [
        "Теперь вывернем наш словарь наизнанку"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcpYyX7BwXAp"
      },
      "source": [
        "topic_text1 = collections.defaultdict(list)\n",
        "for t_t in text_topic.items():\n",
        "  if t_t[1] != []:\n",
        "    topic_text1[t_t[1][0][0]].append(t_t[0]) #топик:[тексты]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1p0EWdZlJ2A"
      },
      "source": [
        "topic_text = dict(topic_text1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVGjKoE62IGP"
      },
      "source": [
        "Для подсчета tf idf и нахождения топ 5 слов с самым высоким значением возьмем код с семинара"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zu9iUIbV-YRT"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEC5MMrpqllU"
      },
      "source": [
        "vectorizer = TfidfVectorizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v66j5b9CVQ-F"
      },
      "source": [
        "def get_top_tf_idf_words(tfidf_vector, feature_names, top_n):\n",
        "    sorted_nzs = np.argsort(tfidf_vector.data)[:-(top_n+1):-1]\n",
        "    return feature_names[tfidf_vector.indices[sorted_nzs]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yV3FvK_2X-2"
      },
      "source": [
        "И пройдемся им по всем топикам"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Agtp156DebZ8"
      },
      "source": [
        "top_words = {}\n",
        "for i in topic_text.keys():\n",
        "  vectors = vectorizer.fit_transform(topic_text[i])\n",
        "  feature_names = np.array(vectorizer.get_feature_names())\n",
        "  for j in range(len(topic_text[i])):\n",
        "    article_vector = vectors[j, :]\n",
        "    words = get_top_tf_idf_words(article_vector, feature_names, 5)\n",
        "    top_words[topic_text[i][j]] = words.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-Xr8Ig-2cqL"
      },
      "source": [
        "Сделаем df и запишем его в файл"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jm3m6o2PvsqT"
      },
      "source": [
        "tw = pd.DataFrame.from_dict(top_words, orient='index')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGTjnweK2tRT",
        "outputId": "7891d54f-2691-4f6b-a937-5b4e5561529f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "tw.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>where thing car nntp_poste host park line wonder could enlighten car see day door sport car look late early call bricklin door really small addition separate rest body know tellme model name engine year production car make history info funky look car mail thank bring neighborhood lerxst</th>\n",
              "      <td>car</td>\n",
              "      <td>door</td>\n",
              "      <td>where</td>\n",
              "      <td>funky</td>\n",
              "      <td>tellme</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>insurance rate performance car line latech newsreader_nnr recently post article ask kind rate single male driver yrs old pay performance car here summary reply receive be anymore close enough twin turbo model ticket accident take defensive driving airbag security single year state farm insurance include additional umbrella policy car house base policy standard policy require defensive drive course less buy car company never accident ticket year quote hope help remember name correctly ask insurance performance car well last year similar situation buy car make inquiry age car drive record turn may insurance go mos also be single incur high rate company have get couple friend pay different in company also maybe be lucky hope info help group be thunderbird sc never make claim insurance though hit several time negligent driver could see stop sign fiddle radio move violation last month go failure clear intersection still say damn light yellow one go go record rate state passive restraint deduction liability deductible comprehensive deductible collision roughly year pay year disclaimer be engineer play work hell thing kill man take away s ever go age group experience may interesting own decide buy gift exotic car front runner include model year narrow like simplicity handling snob appeal drive turbo less money feature performance almost personal luxury car well acceleration high top speed almost ready give buying impulse decide stop insurance agent office way ask would happen rate car buy rate consider year rate safe car slight increase car year new low risk division continue handle account buy change standard high rate company rate double go story well cover rest year much faster actually fast standard make sense book say insure corvette reason underwriter consider supra driver traditional conservative eventually go number reason porsche dealer nice salesman get interested tough high pressure guy back room equal monthly payment would take year longer pay porsche high insurance conclude high insurance relate probability auto theft everyone entitle opinion imagination important knowledge live idaho many year buy insurance year turn immediately drop year accident strictly age change rate stay pretty much sell car pickup year less real amazing thing wake age feel much responsible information single move violation year let see be single male clean driving record pay year good deal ask think get talon think insurance high turbo sport car clean record small town around year age nearby city rate higher have get insure protege year year old state insurance state farm info car co insurance yearly insurance age date license mountain_view move violation hope help post summary possible vijay email single year old turbo full cover reasonable liability ticket violation accident knock wood mass thing make huge difference mass town live be personally good town reasonable distance move best would go move bad would also accident couple ticket would probably add year old ticket go record year full coverage state farm get small discount alarm system year live actually live city price would year be case be interested be insure month s personal total property deductible glass towing state farm drive less year think seriously rip performance car list record clean pay try call insurance dealer could find rate suppose standardize have find place initially call give ridiculously high hit much low also change insurance_companie rate go renewal accident ticket car get old maintain low rate always careful come insurance_companies serge</th>\n",
              "      <td>year</td>\n",
              "      <td>insurance</td>\n",
              "      <td>rate</td>\n",
              "      <td>car</td>\n",
              "      <td>age</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>scsi problem line know specific problem mention message definitely scsi problem drive solution get silverline none loop involve blind write work drive fact work mean</th>\n",
              "      <td>problem</td>\n",
              "      <td>blind</td>\n",
              "      <td>scsi</td>\n",
              "      <td>loop</td>\n",
              "      <td>involve</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>car saftey important line host write safety important factor buy car depend priority lot people put high priority gas mileage cost safety buy unsafe econoboxe instead volvos personally take middle ground thing really look point seatbelt mph bumper figure mph collision brick wall common enough spend much extra money protection lot low speed collision worry</th>\n",
              "      <td>priority</td>\n",
              "      <td>collision</td>\n",
              "      <td>safety</td>\n",
              "      <td>important</td>\n",
              "      <td>mph</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ca integra speed mile positively bad car ever own prelude mile sell still go strong religious attention maintenance oil change car drive exactly manner go clutch underrated set tire really eat tire front even careful align strut start leak burn service note seek stop work radio mile timing belt constant error signal computer finally rod bearing go seriously damage crankshaft contaminate engine overhaul do last week require new crankshaft new cam shaft camshaft shatter try mill camshaft take week national back order engine unique year go new design part expensive way would ever buy acura highly overrate</th>\n",
              "      <td>camshaft</td>\n",
              "      <td>crankshaft</td>\n",
              "      <td>mile</td>\n",
              "      <td>go</td>\n",
              "      <td>week</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                           0  ...        4\n",
              "where thing car nntp_poste host park line wonde...       car  ...   tellme\n",
              "insurance rate performance car line latech news...      year  ...      age\n",
              "scsi problem line know specific problem mention...   problem  ...  involve\n",
              "car saftey important line host write safety imp...  priority  ...      mph\n",
              "ca integra speed mile positively bad car ever o...  camshaft  ...     week\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7FP1DV9xfFX"
      },
      "source": [
        "tw.to_csv('text_top5.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}